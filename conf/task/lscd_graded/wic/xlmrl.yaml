_target_: src.wic.ContextualEmbedder
layers: [1, 12]
layer_aggregation: average
subword_aggregation: average
truncation_tokens_before_target: 0.5
similarity_metric:
  _target_: scipy.spatial.distance.cosine
  _partial_: true

gpu: null
id: xlm-roberta-large

cache:
  _target_: src.wic.contextual_embedder.VectorCache
  metadata:
    dataset:
      name: ${dataset.name}
      version: ${dataset.version}
      preprocessing: ${dataset.preprocessing.name}
    contextual_embedder:
      pre_target_tokens: ${truncation_tokens_before_target}
